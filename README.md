# Face Generation using Deep Convolutional Generative Adversarial Network (DCGAN)

## What's in it

### TLDR
This repository implements an approach of generating human faces using a Deep Convolutional Generative Adversarial Network (DCGAN).
The repository uses the common dataset [CelebA](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset) which consists of more than 200k images of celebrities and a typical architecture of GAN trained on this dataset.
A short introduction to Generative Adversarial Neural Networks is given below.


### Generative Adversarial Networks (GANS)
Generative Adversarial Networks (GAN) are types of artificial neural networks consisting of two main components: a generator and a discriminator network.
The main problem GANs address is the generation of realistic data that follows a specific target distribution like images, text or audio.
The operation of a GAN is characterised by adversarial training in wich the generator and the discriminator are in competition. The generator first generates random data that should follow a specific distribution, e.g. images of faces.
The discriminator then attempts to distinguish between the real data and the data generated by the generator network. During the training process, both the generator and the discriminator continuously adapt to outperform each other.
The generator learns to produce data that is increasingly similar to the real data, while the discriminator learns to distringuish between real and generated data.
This enables GAN network to generate realistic.

### Train Process
The Generative Adversarial Network (GAN) is trained over 10 epochs on a M2 Pro. In each epoch, the discriminator is trained to generate fake images based on some noise. Subsequently, the generator is trained to distinguish between real images and generated images. For the purpose of visualisation, an evaluation step is conducted after the training process. This involves generating a new set of faces using the generator and represents the learning process. You can find these images in this [directory](./data/generated/)

### Evaluation Process
Once the training procedure has been completed, it is necessary to evaluate the performance of the model. The plot in cell 12 displays the losses of the discriminator and generator during the process. Furthermore the notebook implements a method for evaluating the quality of generated images using the Frechet Inception Distance (FID).

#### Frechet Inception Distance
To assess the GANs performance this notebook employs the Frechet Inception Distance (FID) metric. This metric quantifies the similarity between the generated and real distributions of images based on the activation weights of a pretrained network. Consequently, a pretrained inception model is utilized.
A low FID value indicates a high degree of similarity between the generated and the real images.


## Requirements
Follow these steps befor running the notebook

- [ ] python 3.11. or newer installed
- [ ] packages listed in the [requirements.txt](./requirements.txt) file are installed
- [ ] CelebA-Dataset

> **note:** the training process is much faster when using cuda, so maybe setup and install pytorch with cuda (or *mps* on mMacs)

### Directory structure
```
|
|--- data
|   |--- generated
|   |--- CelebA-dataset
|   |--- ...
|
|--- face_generation.ipynb
|
|--- requirements.txt
```
